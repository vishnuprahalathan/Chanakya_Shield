import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import joblib
import numpy as np

print("ğŸ“¥ Loading CICIDS2017 dataset...")
df = pd.read_csv(r"datasets/CICIDS2017_full.csv")
print(f"âœ… Dataset loaded: {df.shape}")


df.columns = df.columns.str.strip()


selected_features = [
    'Destination Port',
    'Flow Duration',
    'Total Fwd Packets',
    'Total Backward Packets',
    'Total Length of Fwd Packets',
    'Total Length of Bwd Packets',
    'Fwd Packet Length Mean',
    'Bwd Packet Length Mean',
    'Flow Packets/s',
    'FIN Flag Count',
    'SYN Flag Count',
    'RST Flag Count',
    'PSH Flag Count',
    'ACK Flag Count',
    'URG Flag Count',
    'CWE Flag Count',
    'ECE Flag Count',
]


available = [col for col in selected_features if col in df.columns]
missing = [col for col in selected_features if col not in df.columns]

print(f"âœ… Using {len(available)} available features.")
if missing:
    print(f"âš ï¸ Missing columns skipped: {missing}")

df = df[available].copy()

print("ğŸ§¹ Cleaning data (handling inf / NaN / extreme values)...")
df.replace([np.inf, -np.inf], np.nan, inplace=True)
df.dropna(inplace=True)
df.reset_index(drop=True, inplace=True)


print(f"âœ… Cleaned dataset shape: {df.shape}")
print(f"ğŸ“Š Feature value ranges:\n{df.describe().T[['min', 'max']]}")


print("âš™ï¸ Scaling feature values...")
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df)

print("ğŸš€ Training Isolation Forest model (unsupervised anomaly detection)...")
model = IsolationForest(
    n_estimators=100,
    contamination=0.1,
    random_state=42
)
model.fit(X_scaled)
print("âœ… Model training complete.")
print(f"ğŸŒ² Number of trees: {len(model.estimators_)}")


joblib.dump(model, "anomaly_model.pkl")
joblib.dump(scaler, "scaler.pkl")
print("ğŸ’¾ Model and scaler saved successfully at:")
print("   â¤ anomaly_model.pkl")
print("   â¤ scaler.pkl")
